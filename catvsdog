import os
import numpy as np
from PIL import Image
from scipy.ndimage import rotate
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt

# 数据集路径配置
DATA_PATH = r"C:\Users\31336\Desktop\dzy\cat-dog-all"
TARGET_SIZE = (150, 150)


# -------------------- 数据加载与预处理 --------------------

def load_images(data_dir=DATA_PATH, target_size=TARGET_SIZE):
    X = []
    y = []

    for label, folder in enumerate(["cat", "dog"]):
        folder_path = os.path.join(data_dir, folder)
        print(f"Loading images from: {folder_path}")

        for filename in os.listdir(folder_path):
            if filename.endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(folder_path, filename)
                try:
                    img = Image.open(img_path).convert('RGB')
                    img = img.resize(target_size)
                    img_array = np.array(img).transpose(2, 0, 1)  # CHW格式
                    X.append(img_array)
                    y.append(label)
                except Exception as e:
                    print(f"Error loading {img_path}: {str(e)}")

    return np.array(X), np.array(y)


def preprocess_data(X, y, test_size=0.2):
    # 归一化
    X = X.astype('float32') / 255.0

    # One-hot编码
    encoder = OneHotEncoder()
    y = encoder.fit_transform(y.reshape(-1, 1)).toarray()

    # 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

    return X_train, X_val, X_test, y_train, y_val, y_test



# -------------------- 改进的数据增强生成器 --------------------

class EnhancedDataGenerator:
    def __init__(self, X, y, batch_size=64, augment=True):
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.augment = augment
        self.indices = np.arange(len(X))
        self.rng = np.random.default_rng(42)

    def __iter__(self):
        self.rng.shuffle(self.indices)
        for start in range(0, len(self.X), self.batch_size):
            end = min(start + self.batch_size, len(self.X))
            batch_idx = self.indices[start:end]
            X_batch = self.X[batch_idx]
            y_batch = self.y[batch_idx]

            if self.augment:
                X_batch = self._augment_batch(X_batch)

            yield X_batch, y_batch

    def _augment_batch(self, batch):
        augmented = []
        for img in batch:
            # 转换为HWC格式进行增强
            img = img.transpose(1, 2, 0)

            # 随机水平翻转
            if self.rng.random() < 0.5:
                img = np.fliplr(img)

            # 随机旋转 (-20到20度)
            if self.rng.random() < 0.5:
                angle = self.rng.uniform(-20, 20)
                img = rotate(img, angle, reshape=False, mode='reflect')

            # 随机裁剪 (80%-100%)
            if self.rng.random() < 0.5:
                h, w = img.shape[:2]
                scale = self.rng.uniform(0.8, 1.0)
                new_h, new_w = int(h * scale), int(w * scale)
                y = self.rng.integers(0, h - new_h)
                x = self.rng.integers(0, w - new_w)
                img = img[y:y + new_h, x:x + new_w]
                img = Image.fromarray((img * 255).astype(np.uint8)).resize((w, h))
                img = np.array(img).astype('float32') / 255.0  # 确保重新归一化

            # 随机颜色调整
            if self.rng.random() < 0.5:
                # 亮度调整
                img = np.clip(img * self.rng.uniform(0.7, 1.3), 0, 1)
                # 对比度调整
                img = np.clip(0.5 + (img - 0.5) * self.rng.uniform(0.7, 1.3), 0, 1)

            # 转回CHW格式
            augmented.append(img.transpose(2, 0, 1))

        return np.array(augmented)

    def __len__(self):
        return len(self.X) // self.batch_size


# -------------------- 改进的模型组件 --------------------

class Conv2D:
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        # He初始化
        he_std = np.sqrt(2.0 / (in_channels * kernel_size ** 2))
        self.weights = he_std * np.random.randn(out_channels, in_channels, kernel_size, kernel_size)
        self.bias = np.zeros((out_channels, 1))
        self.stride = stride
        self.padding = padding

    def forward(self, X):
        self.X = np.pad(X, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)),
                        mode='constant')
        batch, _, in_h, in_w = self.X.shape
        out_h = (in_h - self.weights.shape[2]) // self.stride + 1
        out_w = (in_w - self.weights.shape[3]) // self.stride + 1

        self.output = np.zeros((batch, self.weights.shape[0], out_h, out_w))
        for i in range(out_h):
            for j in range(out_w):
                h_start = i * self.stride
                w_start = j * self.stride
                receptive_field = self.X[:, :, h_start:h_start + self.weights.shape[2],
                                  w_start:w_start + self.weights.shape[3]]
                self.output[:, :, i, j] = np.tensordot(
                    receptive_field, self.weights, axes=([1, 2, 3], [1, 2, 3])
                ) + self.bias.T
        return self.output

    def backward(self, dout):
        batch, _, out_h, out_w = dout.shape
        dX = np.zeros_like(self.X)
        dW = np.zeros_like(self.weights)
        dB = np.zeros_like(self.bias)

        for i in range(out_h):
            for j in range(out_w):
                h_start = i * self.stride
                w_start = j * self.stride
                receptive_field = self.X[:, :, h_start:h_start + self.weights.shape[2],
                                  w_start:w_start + self.weights.shape[3]]

                dout_slice = dout[:, :, i, j][:, :, None, None, None]
                dW += np.sum(receptive_field[:, None, :, :, :] * dout_slice, axis=0)
                dB += np.sum(dout[:, :, i, j], axis=0)[:, None]
                dX[:, :, h_start:h_start + self.weights.shape[2], w_start:w_start + self.weights.shape[3]] += np.sum(
                    self.weights[None, :, :, :, :] * dout_slice, axis=1
                )

        if self.padding > 0:
            dX = dX[:, :, self.padding:-self.padding, self.padding:-self.padding]
        return dX, dW, dB


class BatchNorm2D:
    def __init__(self, num_features, momentum=0.9, eps=1e-5):
        self.gamma = np.ones((1, num_features, 1, 1))
        self.beta = np.zeros((1, num_features, 1, 1))
        self.momentum = momentum
        self.eps = eps
        self.running_mean = np.zeros((1, num_features, 1, 1))
        self.running_var = np.ones((1, num_features, 1, 1))
        self.X_centered = None
        self.std_inv = None

    def forward(self, X, training=True):
        if training:
            batch_mean = X.mean(axis=(0, 2, 3), keepdims=True)
            batch_var = X.var(axis=(0, 2, 3), keepdims=True)
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var
        else:
            batch_mean = self.running_mean
            batch_var = self.running_var

        self.X_centered = X - batch_mean
        self.std_inv = 1.0 / np.sqrt(batch_var + self.eps)
        self.X_norm = self.X_centered * self.std_inv
        return self.gamma * self.X_norm + self.beta

    def backward(self, dout):
        dgamma = np.sum(dout * self.X_norm, axis=(0, 2, 3), keepdims=True)
        dbeta = np.sum(dout, axis=(0, 2, 3), keepdims=True)

        N = dout.shape[0] * dout.shape[2] * dout.shape[3]
        dX_centered = dout * self.gamma * self.std_inv
        dvar = np.sum(dX_centered * self.X_centered, axis=(0, 2, 3), keepdims=True) * (-0.5) * (self.std_inv ** 3)
        dmean = -np.sum(dX_centered + (2.0 / N) * dvar * self.X_centered, axis=(0, 2, 3), keepdims=True)

        dX = dX_centered + (2.0 / N) * dvar * self.X_centered + (1.0 / N) * dmean
        return dX, dgamma, dbeta


class AdvancedCNN:
    def __init__(self):
        # 卷积块1
        self.conv1 = Conv2D(3, 32)
        self.bn1 = BatchNorm2D(32)
        self.relu = lambda x: np.maximum(0, x)
        self.pool1 = MaxPooling2D()  # 独立池化实例
        self.dropout1 = Dropout(0.3)

        # 卷积块2
        self.conv2 = Conv2D(32, 64)
        self.bn2 = BatchNorm2D(64)
        self.pool2 = MaxPooling2D()  # 独立池化实例
        self.dropout2 = Dropout(0.4)

        # 卷积块3
        self.conv3 = Conv2D(64, 128)
        self.bn3 = BatchNorm2D(128)
        self.pool3 = MaxPooling2D()  # 独立池化实例
        self.dropout3 = Dropout(0.4)

        # 卷积块4
        self.conv4 = Conv2D(128, 256)
        self.bn4 = BatchNorm2D(256)
        self.pool4 = MaxPooling2D()  # 独立池化实例
        self.dropout4 = Dropout(0.5)

        # 全连接层
        self.flatten = lambda x: x.reshape(x.shape[0], -1)
        self.fc1 = Dense(256 * 9 * 9, 512)
        self.dropout_fc = Dropout(0.6)
        self.fc2 = Dense(512, 2)
        self.softmax = Softmax()

        # 参数列表
        self.params = [
            self.conv1.weights, self.conv1.bias, self.bn1.gamma, self.bn1.beta,
            self.conv2.weights, self.conv2.bias, self.bn2.gamma, self.bn2.beta,
            self.conv3.weights, self.conv3.bias, self.bn3.gamma, self.bn3.beta,
            self.conv4.weights, self.conv4.bias, self.bn4.gamma, self.bn4.beta,
            self.fc1.weights, self.fc1.bias,
            self.fc2.weights, self.fc2.bias
        ]

    def forward(self, X, training=True):
        # 保存原始输入用于反向传播
        self.input_shape = X.shape

        # 卷积块1
        x = self.conv1.forward(X)
        x = self.bn1.forward(x, training)
        x = self.relu(x)
        x = self.pool1.forward(x)  # 使用独立池化实例
        x = self.dropout1.forward(x, training)

        # 卷积块2
        x = self.conv2.forward(x)
        x = self.bn2.forward(x, training)
        x = self.relu(x)
        x = self.pool2.forward(x)  # 使用独立池化实例
        x = self.dropout2.forward(x, training)

        # 卷积块3
        x = self.conv3.forward(x)
        x = self.bn3.forward(x, training)
        x = self.relu(x)
        x = self.pool3.forward(x)  # 使用独立池化实例
        x = self.dropout3.forward(x, training)

        # 卷积块4
        x = self.conv4.forward(x)
        x = self.bn4.forward(x, training)
        x = self.relu(x)
        x = self.pool4.forward(x)  # 使用独立池化实例
        x = self.dropout4.forward(x, training)

        # 全连接层
        x = self.flatten(x)
        x = self.fc1.forward(x)
        x = self.relu(x)
        x = self.dropout_fc.forward(x, training)
        x = self.fc2.forward(x)
        return self.softmax.forward(x)

    def backward(self, y_true, y_pred):
        # 计算梯度
        loss_grad = (y_pred - y_true) / y_true.shape[0]

        # 反向传播全连接层
        dout = self.fc2.backward(loss_grad)[0]
        dout = self.dropout_fc.backward(dout)
        dout = self.fc1.backward(dout)[0]

        # 将梯度重塑回卷积块4的输出形状
        dout = dout.reshape(self.dropout4.mask.shape)  # 使用保存的dropout形状

        # 反向传播卷积块4
        dout = self.dropout4.backward(dout)
        dout = self.pool4.backward(dout)  # 对应独立池化实例
        dout = self.relu_grad(dout, self.bn4.X_norm)
        dout, _, _ = self.bn4.backward(dout)
        dout, _, _ = self.conv4.backward(dout)

        # 反向传播卷积块3
        dout = self.dropout3.backward(dout)
        dout = self.pool3.backward(dout)  # 对应独立池化实例
        dout = self.relu_grad(dout, self.bn3.X_norm)
        dout, _, _ = self.bn3.backward(dout)
        dout, _, _ = self.conv3.backward(dout)

        # 反向传播卷积块2
        dout = self.dropout2.backward(dout)
        dout = self.pool2.backward(dout)  # 对应独立池化实例
        dout = self.relu_grad(dout, self.bn2.X_norm)
        dout, _, _ = self.bn2.backward(dout)
        dout, _, _ = self.conv2.backward(dout)

        # 反向传播卷积块1
        dout = self.dropout1.backward(dout)
        dout = self.pool1.backward(dout)  # 对应独立池化实例
        dout = self.relu_grad(dout, self.bn1.X_norm)
        dout, _, _ = self.bn1.backward(dout)
        dout, _, _ = self.conv1.backward(dout)

        # 收集所有梯度
        gradients = [
            self.conv1.weights, self.conv1.bias,
            self.bn1.gamma, self.bn1.beta,
            self.conv2.weights, self.conv2.bias,
            self.bn2.gamma, self.bn2.beta,
            self.conv3.weights, self.conv3.bias,
            self.bn3.gamma, self.bn3.beta,
            self.conv4.weights, self.conv4.bias,
            self.bn4.gamma, self.bn4.beta,
            self.fc1.weights, self.fc1.bias,
            self.fc2.weights, self.fc2.bias
        ]
        return gradients

    def relu_grad(self, dout, x):
        """ReLU梯度计算"""
        return dout * (x > 0).astype(np.float32)


# -------------------- 其他组件保持不变 --------------------

class MaxPooling2D:
    def __init__(self, pool_size=2, stride=2):
        self.pool_size = pool_size
        self.stride = stride

    def forward(self, X):
        self.X = X
        batch, channels, h, w = X.shape
        out_h = h // self.stride
        out_w = w // self.stride

        self.output = np.zeros((batch, channels, out_h, out_w))
        for i in range(out_h):
            for j in range(out_w):
                h_start = i * self.stride
                w_start = j * self.stride
                self.output[:, :, i, j] = np.max(
                    X[:, :, h_start:h_start + self.pool_size, w_start:w_start + self.pool_size],
                    axis=(2, 3)
                )
        return self.output

    def backward(self, dout):
        dX = np.zeros_like(self.X)
        for i in range(dout.shape[2]):
            for j in range(dout.shape[3]):
                h_start = i * self.stride
                w_start = j * self.stride
                pool_region = self.X[:, :, h_start:h_start + self.pool_size, w_start:w_start + self.pool_size]
                max_mask = (pool_region == np.max(pool_region, axis=(2, 3), keepdims=True))
                dX[:, :, h_start:h_start + self.pool_size, w_start:w_start + self.pool_size] += (
                        max_mask * dout[:, :, i:i + 1, j:j + 1]
                )
        return dX


class Dense:
    def __init__(self, input_size, output_size):
        # He初始化
        he_std = np.sqrt(2.0 / input_size)
        self.weights = he_std * np.random.randn(output_size, input_size)
        self.bias = np.zeros((output_size, 1))

    def forward(self, X):
        self.X = X
        return X @ self.weights.T + self.bias.T

    def backward(self, dout):
        dX = dout @ self.weights
        dW = dout.T @ self.X
        dB = np.sum(dout, axis=0, keepdims=True).T
        return dX, dW, dB


class Dropout:
    def __init__(self, p=0.5):
        self.p = p
        self.mask = None

    def forward(self, X, training=True):
        if training:
            self.mask = (np.random.rand(*X.shape) < (1 - self.p)) / (1 - self.p)
            return X * self.mask
        return X

    def backward(self, dout):
        return dout * self.mask


class Softmax:
    def forward(self, X):
        exps = np.exp(X - np.max(X, axis=1, keepdims=True))
        return exps / np.sum(exps, axis=1, keepdims=True)


class AdamOptimizer:
    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.m = [np.zeros_like(p) for p in params]
        self.v = [np.zeros_like(p) for p in params]
        self.t = 0

    def step(self, params, grads):
        self.t += 1
        for i in range(len(params)):
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads[i]
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grads[i] ** 2)

            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)

            params[i] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)


# -------------------- 改进的训练流程 --------------------

def train():
    # 加载数据
    print("Loading data...")
    X, y = load_images()
    X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(X, y)

    # 创建数据生成器
    train_gen = EnhancedDataGenerator(X_train, y_train, batch_size=64)
    val_gen = EnhancedDataGenerator(X_val, y_val, batch_size=64, augment=False)

    # 初始化模型和优化器
    model = AdvancedCNN()
    optimizer = AdamOptimizer(model.params, lr=0.001)  # 更小的初始学习率

    best_val_acc = 0
    best_params = None
    early_stop_counter = 0
    patience = 4

    print("Starting training...")
    for epoch in range(20):
        # 训练阶段
        model.train = True
        train_loss = 0
        for X_batch, y_batch in train_gen:
            # 前向传播
            y_pred = model.forward(X_batch)

            # 计算损失
            loss = -np.mean(y_batch * np.log(y_pred + 1e-8))
            train_loss += loss

            # 反向传播
            gradients = model.backward(y_batch, y_pred)
            optimizer.step(model.params, gradients)

        # 验证阶段
        model.train = False
        val_loss = 0
        val_correct = 0
        total = 0
        for X_val_batch, y_val_batch in val_gen:
            y_val_pred = model.forward(X_val_batch)
            val_loss += -np.mean(y_val_batch * np.log(y_val_pred + 1e-8))
            val_correct += np.sum(np.argmax(y_val_pred, axis=1) == np.argmax(y_val_batch, axis=1))
            total += len(X_val_batch)
        val_acc = val_correct / total

        # 学习率调整
        if epoch > 10 and val_acc < best_val_acc:
            optimizer.lr *= 0.5  # 当性能下降时降低学习率

        # 保存最佳模型
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            early_stop_counter = 0
            best_params = [p.copy() for p in model.params]
        else:
            early_stop_counter += 1

        print(f"Epoch {epoch + 1}, "
              f"Train Loss: {train_loss / len(train_gen):.4f}, "
              f"Val Loss: {val_loss / len(val_gen):.4f}, "
              f"Val Acc: {val_acc:.4f}")

        if early_stop_counter >= patience:
            print("Early stopping...")
            break

    # 绘制损失曲线
    plt.figure(figsize=(10, 6))
    plt.plot(train_loss, label='Training Loss', marker='o')
    plt.plot(val_loss, label='Validation Loss', marker='s', linestyle='--')
    plt.title('Training and Validation Loss Curves')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.axvline(x=patience, color='r', linestyle='--', label=f'Early Stop ({patience} Epochs)')
    plt.show()


    # 加载最佳参数
    if best_params is not None:
        for i in range(len(model.params)):
            model.params[i][...] = best_params[i]

    # 测试阶段
    model.train = False
    test_correct = 0
    total = 0
    test_gen = EnhancedDataGenerator(X_test, y_test, batch_size=64, augment=False)
    for X_test_batch, y_test_batch in test_gen:
        y_test_pred = model.forward(X_test_batch)
        test_correct += np.sum(np.argmax(y_test_pred, axis=1) == np.argmax(y_test_batch, axis=1))
        total += len(X_test_batch)
    test_acc = test_correct / total
    print(f"Final Test Accuracy: {test_acc:.4f}")


if __name__ == "__main__":
    train()
